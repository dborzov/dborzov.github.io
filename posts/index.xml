<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Dmitry Peter Borzov</title>
    <link>http://www.borzov.ca/posts/</link>
    <description>Recent content in Posts on Dmitry Peter Borzov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Jul 2016 08:36:54 -0700</lastBuildDate>
    <atom:link href="http://www.borzov.ca/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deriving asymptotics for quicksort</title>
      <link>http://www.borzov.ca/posts/quicksort/</link>
      <pubDate>Tue, 05 Jul 2016 08:36:54 -0700</pubDate>
      
      <guid>http://www.borzov.ca/posts/quicksort/</guid>
      <description>

&lt;h1 id=&#34;deriving-asymptotics-for-quicksort:58862ac77588df2b663ea23230d5232f&#34;&gt;Deriving asymptotics for quicksort&lt;/h1&gt;

&lt;h3 id=&#34;abstract-building-intuition-for-the-processing-time-asymptotic-behaviour-for-quicksort:58862ac77588df2b663ea23230d5232f&#34;&gt;&lt;em&gt;Abstract:&lt;/em&gt; Building intuition for the processing time asymptotic behaviour for quicksort&lt;/h3&gt;

&lt;p&gt;Randomized algorithms are fascinating and somewhat misterious: they can deliver surprising efficiency but being able to actually understand what gives them their properties can be pretty hard at times.&lt;/p&gt;

&lt;p&gt;Specifically, deriving the execution time estimation can get pretty challenging in general case, in a sharp contrast to the determenistic algorithms. For the average execution time, for example, we have to consider each of the possible random outcomes along with its probability and then average out over all of them. Not to mention that just the average time may not enough to characterize the alorithm and we may have to investigate metrics beyond just the average. In fact, the mathematical expressions behind those easily gets so hairy that they tend to be omitted at all in many educational materials and textbooks.&lt;/p&gt;

&lt;p&gt;However, that complexity is also a bit deceptive in practice. The keys is that we are usually only interested in asymptotics, that is the behaviour in some limit, such as the limit of large dataset. That simplifies the problem immensely and means that, with a bit of experience behind our belt, we can learn to arrive at the answer pretty easily. It all comes down to just carefully studying what terms are important in our case of interest and what are not.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s illustrate this with an example. We will look at the asymptotics of what is probably the most famous random-based algorithm: the quicksort. We will be interested in the asymptotic limit of large arrays: that is, when the number of elements &lt;code&gt;n&lt;/code&gt; in the array we need to sort gets pretty big. Let&amp;rsquo;s denote this time with &lt;code&gt;f(n)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The challenge here is that we don&amp;rsquo;t know how lucky we are going to get with our random choice of the pivot element: it could be the median which means we divided our problem into two smaller ones perfectly, or it could be the smallest element and we may have shifted the whole array to only &amp;ldquo;sort&amp;rdquo; one element.&lt;/p&gt;

&lt;h2 id=&#34;boundaries:58862ac77588df2b663ea23230d5232f&#34;&gt;Boundaries&lt;/h2&gt;

&lt;p&gt;Why do we need to go through the deriviation at all? Unless you are working in alorithms research, it is unlikely that we would ever . Numerical simulation is good enough. My reason is that it helps build a proper understanding of what is going on: see which decisions in our design matter and which are not.&lt;/p&gt;

&lt;p&gt;That is why the best thing is to look into the custom cases what kind of answer we may reasonably expect.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the worst case &lt;code&gt;f_{worst}(n)&lt;/code&gt;: this case unlikiest ever: every time we manage to make the worst pivot element ever: the smaller subset only has the pivot element, and the other one contains all the other ones, &lt;code&gt;n-1&lt;/code&gt;. We then we have to make the recursive call on that set.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;f_{worst}(n) = f_{worst}(n-1) + n&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We can just unfold the expression recursively:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;f_{worst}(n) = f_{worst}(n-1) + n = f_{worst}(n-2) + (n-1) + n = f_{worst}(1) + \sum_{x=1..n} x&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Since we are only interested in asymptotics we don&amp;rsquo;t even need to be careful with the arithmetic sum expression  &lt;code&gt;\sum_{x=1..n} x&lt;/code&gt;. We can just apply the fact that with the paring up of the elements, the average is about &lt;code&gt;\frac{x}{2}&lt;/code&gt; and we get &lt;code&gt;x&lt;/code&gt; of them, so &lt;code&gt;f_{worst}(n) = n^2&lt;/code&gt;. In the worst case, quicksort looks like the naive sort implementation: we keep selecting the smallest (or the biggest) element of the array, insert it into the very beginning (or end).&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the best case: when every pivot we choose turns out to be the median element of the array! The two subsets are then exactly half in size:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;f_{best}(n) = 2 \cdot f_{best}(\frac{n}{2}) + n&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enabling cls/Cmd&#43;K for Cygwin Bash on ConEmu</title>
      <link>http://www.borzov.ca/posts/cls4cygwin/</link>
      <pubDate>Thu, 03 Dec 2015 19:28:54 -0700</pubDate>
      
      <guid>http://www.borzov.ca/posts/cls4cygwin/</guid>
      <description>&lt;p&gt;Sometimes I have to use a shell (a command-line interface) on Windows and overall it is pretty challenging for me.
Windows is fundamentally different from the Unix/POSIX conventions I got used to when working with Linux and MacOSX and the native shells  for Windows (cmd/Powershell) are quite different.&lt;/p&gt;

&lt;p&gt;Fornunatelly, there is Cygwin, an open-source project that wraps the Windows functioanllity and makes some imitation of a bash we came to know and love that offers something similar to &lt;a href=&#34;en.wikipedia.org/wiki/Cygwin&#34;&gt;bash&lt;/a&gt;. It is pretty decent for most every day uses and I like it a lot.&lt;/p&gt;

&lt;p&gt;One thing that helps is a decent Terminal application (the thing that provides GUI for the cli session). &lt;a href=&#34;https://github.com/Maximus5/ConEmu&#34;&gt;ConEmu&lt;/a&gt; is pretty great, highly customizable and has all the basic features.&lt;/p&gt;

&lt;p&gt;One thing I missed from iTerm2 is to be able to clear the session screen completely with &amp;ldquo;Cmd+K&amp;rdquo; letters. I got investigating and I am happy to report that I found the solution.&lt;/p&gt;

&lt;p&gt;It turns out that there is another key difference between Unix and Windows shells: in Unix, keeping track of printed content is the responsibility of the terminal application, but for Windows it is a part of the shell&amp;rsquo;s session. As a result, clearing the screen can be done with a shell command, &lt;code&gt;cls&lt;/code&gt; in their case.&lt;/p&gt;

&lt;p&gt;So since Cygwin is still a Windows shell, clearing screen is a shell command as well, which is invoked with an escape code. Here it is: &lt;code&gt;echo -e &#39;\0033\0143&#39;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is how to set it up:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.borzov.ca/conemu.png&#34; alt=&#34;Img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s set up ConEmu&amp;rsquo;s hot key to use the command. Open up the ConEmu settings (right click on the button)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding routing in Bittorrents: algorithms behind Kademlia DHT</title>
      <link>http://www.borzov.ca/posts/kademlia/</link>
      <pubDate>Fri, 30 Jan 2015 08:36:54 -0700</pubDate>
      
      <guid>http://www.borzov.ca/posts/kademlia/</guid>
      <description>

&lt;h1 id=&#34;routing-at-bittorrent-understanding-kademlia-dhts-distributed-hash-tables:fe965123d7524294753e9acd8d3c8d0a&#34;&gt;Routing at Bittorrent: understanding Kademlia DHTs (Distributed Hash Tables)&lt;/h1&gt;

&lt;h3 id=&#34;unfinished-draft:fe965123d7524294753e9acd8d3c8d0a&#34;&gt;(unfinished  draft)&lt;/h3&gt;

&lt;p&gt;That article is currently work in progress and will probably take a while to finish.&lt;/p&gt;

&lt;p&gt;DHTs (Distributed Hash Tables) protocols are beautiful and practical and I am convinced that we as a species are only beginning to untap their true potential. Here is a good introduction to the core concepts behind the DHT Networks: &lt;a href=&#34;http://www.freedomlayer.org/articles/dht_intro.html&#34;&gt;www.freedomlayer.org/articles/dht_intro.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That intro introduces Chord DHT protocol as a somewhat  simpler case. In practice, many applications for untrusted networks use Kademlia, including the most succesful one to date: trackerless Bitttorrent file sharing.&lt;/p&gt;

&lt;p&gt;Here I am going to use NetVis, an open source network visualizer framework, to visualize in detail how Kademlia protocol operates.&lt;/p&gt;

&lt;p&gt;We will review and annotate a Kademlia-style DHT implementation from the IPFS project source code as an example.&lt;/p&gt;

&lt;p&gt;IPFS - the Interplanetary File System is an amazing project that combines best and proved approaches behind git and gittorrent and you should &lt;a href=&#34;https://github.com/jbenet/ipfs&#34;&gt;totally learn about it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While IPFS is an extensive project and combines many concepts, all we need to understand  for our purposes is that DHT there serves as  a routing purpose in the way similar to Bittorrent file swarm.&lt;/p&gt;

&lt;p&gt;That is, DHT network is used to fetch content stored at other nodes.  Content is identified by a unique hash. The DHT stores these content block hashes as keys and addresses of the nodes that have these content blocks as values.&lt;/p&gt;

&lt;p&gt;All that is implemented within the &lt;a href=&#34;https://godoc.org/github.com/jbenet/go-ipfs/routing&#34;&gt;routing/dht&lt;/a&gt; IPFS subpackage.&lt;/p&gt;

&lt;p&gt;DHT routing implement the general routing interface defined at &lt;a href=&#34;https://github.com/jbenet/go-ipfs/blob/9dd12922b341d891a2365beb10d0142fd10fb235/routing/routing.go&#34;&gt;&lt;code&gt;routing/routing.go&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;	type IpfsRouting interface {
	    FindProvidersAsync(context.Context, u.Key, int) &amp;lt;-chan peer.PeerInfo

	    // PutValue adds value corresponding to given Key.
	    PutValue(context.Context, u.Key, []byte) error

	    // GetValue searches for the value corresponding to given Key.
	    GetValue(context.Context, u.Key) ([]byte, error)

	    // Announce that this node can provide value for given key
	    Provide(context.Context, u.Key) error

	    // Find specific Peer
	    // FindPeer searches for a peer with given ID, returns a peer.PeerInfo
	    // with relevant addresses.
	    FindPeer(context.Context, peer.ID) (peer.PeerInfo, error)

	    // Ping a peer, log the time it took
	    Ping(context.Context, peer.ID) (time.Duration, error)

	    // Bootstrap allows callers to hint to the routing system to get into a
	    // Boostrapped state
	    Bootstrap(context.Context) error
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That corresponds to
the folowing key operations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PutValue&lt;/strong&gt; - we put an entry into the hash table with &lt;code&gt;u.Key&lt;/code&gt; as the key, and arbirtary &lt;code&gt;[]byte&lt;/code&gt; as the value&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GetValue&lt;/strong&gt; - we resolve the key from the hash table&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provide&lt;/strong&gt;  - A node anounces to DHT that it can serve a specific key&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FindPeer&lt;/strong&gt; - - we search for a peer node within the DHT network by its ID ( IPFS is a complicated project with a lot of concepts and they try to stick to a  consistent nomenclature. As they have plenty of graphs wit nodes, they call the DHT network nodes peers everywhere accross the project).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The DHT package at &lt;code&gt;routing/dht&lt;/code&gt; defines &lt;code&gt;IpfsDHT struct&lt;/code&gt; that implements that interface.&lt;/p&gt;

&lt;p&gt;Here are the links we will ned in order to explore that implementation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://sourcegraph.com/github.com/jbenet/go-ipfs@master/.tree/routing/dht/dht.go&#34;&gt;The package at Sourcegraph.com&lt;/a&gt;, allows source code browsing where you can click on an item to see its definition (just like a proper IDE)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://godoc.org/github.com/jbenet/go-ipfs/routing/dht&#34;&gt;godoc for the package&lt;/a&gt;, generated package documentation: signatures and docstrings of the public elements&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Predecessor search for Big Data: x-fast tries, locality of reference and all that</title>
      <link>http://www.borzov.ca/posts/xfast/</link>
      <pubDate>Sun, 01 Dec 2013 08:36:54 -0700</pubDate>
      
      <guid>http://www.borzov.ca/posts/xfast/</guid>
      <description>

&lt;h1 id=&#34;predecessor-search-for-big-data-x-fast-tries-locality-of-reference-and-all-that:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Predecessor search for Big Data: x-fast tries, locality of reference and all that&lt;/h1&gt;

&lt;h3 id=&#34;abstract-an-intro-to-data-structures-with-locality-of-reference-type-features-we-review-x-fast-tries-in-some-detail-and-then-talk-about-similar-data-structures-where-they-shine-and-where-they-don-t:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;&lt;em&gt;Abstract:&lt;/em&gt; An intro to data structures with locality of reference-type features. We review x-fast tries in some detail and then talk about similar data structures, where they shine and where they don&amp;rsquo;t&lt;/h3&gt;

&lt;p&gt;Consider a set &lt;code&gt;{n}&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; integers within the range &lt;code&gt;0..u-1&lt;/code&gt;. Let&amp;rsquo;s assume we need to build a data structure that allows for performing of the following operations efficiently:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Look up&lt;/strong&gt;: check if an integer &lt;code&gt;x&lt;/code&gt; is in &lt;code&gt;{n}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt;: Insert an integer into &lt;code&gt;{n}&lt;/code&gt; or delete one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s check out how some common data structures are holding up with these: a simple sorted array, a balanced binary search tree (whether it is AVL or red-black), and  a hash table:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Lookup&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sorted Array&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Balanced binary tree&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash Table&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The hash table is designed to be optimal for these two operations so it is the best fit here. But what if we modify the search operation a little bit and  require returning of the next closest element if the exact value is not within  the set &lt;code&gt;{n}&lt;/code&gt;? So that for 69 in {1,55,68, 100} we would want to get 68 as it is close enough. One can see how the need for such a modification can arise in applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.borzov.ca/img/arrow.png&#34; alt=&#34;Img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can think of a couple of similar problem modifications: return the closest element, or only the next closest element that is greater than the query value (they call this the successor element search in the literature), or the closest one that is smaller (predecessor search). One can see that these problems overlap and can be reformulated in terms of each other, so let&amp;rsquo;s just start with implementing one  of them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Find predecessor&lt;/strong&gt;  item: look up an integer &lt;code&gt;x&lt;/code&gt; and if it is not in the set, return the closest int within the set that is lower by value&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is how we can implement Predecessor Search for the data structures we considered above:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sorted Array&lt;/strong&gt; we search for the value using binary search and if turns out that it is not within the set, we simply take the next left value from where it should have been.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Search Tree&lt;/strong&gt; approach is similar to &lt;strong&gt;Sorted Array&lt;/strong&gt;: we traverse to the next item left from where the value should have been&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hash Table&lt;/strong&gt;: if the value &lt;code&gt;x&lt;/code&gt; is not within the set, the only option is to look up &lt;code&gt;x-1&lt;/code&gt; and continue looking up lower values until we stumble upon one within the set. The average distance between the two values is asymptotically &lt;code&gt;Ө(u/n)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Lookup&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Predeccessor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sorted Array&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Balanced binary tree&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash Table&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;O(u/n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We see that the advantages of the plain hash table disappear when it comes to predecessor search. Can we think of a way to improve the hash table&amp;rsquo;s performance here?&lt;/p&gt;

&lt;p&gt;One way that may come to mind at this point is to just store pointers to predecessor and successor elements for each of the the elements within &lt;code&gt;u&lt;/code&gt; right in the hash table.&lt;/p&gt;

&lt;p&gt;This would enable quick predecessor lookup, but would break the performance of the Update function. Adding a value for this Hash-Table with Predecessor Pointers means we need to update the predecessor pointers for all values from the new one to the closest larger one. Which, again, means O(u/n) operations.&lt;/p&gt;

&lt;p&gt;There is another drawback to this approach. Now we have to store not just &lt;code&gt;n&lt;/code&gt; set elements but all the &lt;code&gt;u&lt;/code&gt; possible values. Additionally, pointer size is bound by &lt;code&gt;log(u)&lt;/code&gt; needed to store the value position. So the total size of the structure now depends on &lt;code&gt;u&lt;/code&gt; as well.&lt;/p&gt;

&lt;p&gt;We can sum it up with this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Lookup&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Predeccessor&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sorted Array&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Balanced binary tree&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash Table&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(u/n)&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash-Table with Predecessor Pointers&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(u/n)&lt;/td&gt;
&lt;td&gt;Ө(u log(u))&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;some-reflection:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Some Reflection&lt;/h4&gt;

&lt;p&gt;Let&amp;rsquo;s reflect on these results a little bit. We see that hash table is ideal when we look up exact values but breaks down when we start inquiring on operations where &lt;em&gt;locality&lt;/em&gt; of the value starts to matter.&lt;/p&gt;

&lt;p&gt;That makes sense. A proper hash function means a &lt;a href=&#34;http://en.wikipedia.org/wiki/Numerical_stability&#34;&gt;mathematically unstable&lt;/a&gt; one: even small increments change the hash completely which insures the values are spread homogeneously among the hash table buckets.&lt;/p&gt;

&lt;p&gt;At the same time we see that binary search tree-style data structures are quite resielent to &lt;em&gt;locality&lt;/em&gt; context class of problem. One can say that search tree is based on the concept of proximity and contains the information about &lt;em&gt;locality&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;X-fast trie is a data structure that combines the advantages of both search trees and hash tables.&lt;/p&gt;

&lt;h4 id=&#34;enter-x-fast-trie:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Enter X-fast trie&lt;/h4&gt;

&lt;p&gt;Let&amp;rsquo;s build a bitwise search tree on top of all the &lt;code&gt;u&lt;/code&gt; elements. All the &lt;code&gt;u&lt;/code&gt; values are at the tree&amp;rsquo;s leaves (which makes this search tree a trie). Let&amp;rsquo;s mark all those tree nodes where there is an ancestor leave which is within &lt;code&gt;{n}&lt;/code&gt; as &amp;ldquo;black&amp;rdquo; and the rest of the nodes keep &amp;ldquo;white&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Here is a nice illustrating graph from &lt;a href=&#34;http://opendatastructures.org/versions/edition-0.1c/ods-java/node66.html&#34;&gt;opendatastructures.org&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://opendatastructures.org/versions/edition-0.1c/ods-java/img1455.png&#34; alt=&#34;x-fast&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we can implement the operations like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Predecessor&lt;/strong&gt;: there is &lt;code&gt;log(u)&lt;/code&gt; nodes within the search tree that are parents to the leave corresponding to the value we look up. If a node is marked with black all the nodes above are black too. That means we have a sorted array of &lt;code&gt;log(u)&lt;/code&gt; values corresponding to the &lt;code&gt;i&lt;/code&gt; element. We use binary search to find the lowest black element and then traverse down the left side along the black nodes to get to the predecessor value. How much will it cost? Binary search among &lt;code&gt;log(u)&lt;/code&gt; values would be &lt;code&gt;Ө(log(log(u)))&lt;/code&gt;, traversing down to predecessor value: &lt;code&gt;Ө(1)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: is quite similar to the simple trie update case. For adding a value, ee traverse through all the parent nodes and &amp;ldquo;repaint&amp;rdquo; all of them black. For removing an item, we paint with white all the parent nodes for which no other children leaves are black. So &lt;code&gt;Ө(log(u))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Memory needed to store the &amp;ldquo;color&amp;rdquo; (black or white) bits for all the nodes within the search tree grows steeply: we get &lt;code&gt;2u-1&lt;/code&gt; nodes for a complete binary trie with u leaves. That is where a hash table comes in, we only the &amp;ldquo;black&amp;rdquo;-marked nodes within the hash table for each level of the trie. That means &lt;code&gt;n&lt;/code&gt; entries on the leave level, less or equal than &lt;code&gt;n/2&lt;/code&gt; on the second lowest and so on for each of the &lt;code&gt;log(u)&lt;/code&gt; hash tables. So the space is  bound by &lt;code&gt;Ө(n log(u))&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s sum it up by updating the table&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Lookup&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Predecessor&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sorted Array&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Balanced binary tree&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;Ө(log(n))&lt;/td&gt;
&lt;td&gt;O(log(n))&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash Table&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(M/n)&lt;/td&gt;
&lt;td&gt;Ө(n)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hash-Table with Predecessor Pointers&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(1)&lt;/td&gt;
&lt;td&gt;Ө(u/n)&lt;/td&gt;
&lt;td&gt;Ө(u log(u))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;X-fast trie&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;O(log(u))&lt;/td&gt;
&lt;td&gt;O(log(log(u)))&lt;/td&gt;
&lt;td&gt;O(n log(u))&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;toy-example:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Toy example&lt;/h4&gt;

&lt;p&gt;The easiest way to grok how these operations are implemented is to implement them yourself. So I &lt;a href=&#34;https://github.com/dborzov/XLtrie&#34;&gt;did&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is our toy case. With movie and tv show titles such as X-files, X-men and American History X, we have a trope of X standing for something misterious (and heavily implied to be hip). How often are movies with such titles released? What was the first X-movie released since, say 1975?&lt;/p&gt;

&lt;p&gt;To find out, let us build the X-fast trie index for such X-titled movies by the year of release.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider years from 1950 + 64 = 2004 to be our M= 2^6 - 1=63 range.
Here is a plot of what all the M leaves and the color of parent nodes for each of the depth levels:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.borzov.ca/img/movies.png&#34; alt=&#34;Img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We have total of &lt;code&gt;x&lt;/code&gt; levels, where x comes from $u &amp;lt; 2^x -1$. That means we have got &lt;code&gt;x&lt;/code&gt; hash functions where we store keys of the &amp;ldquo;black&amp;rdquo;-marked nodes. Root level 0 contains only one node, in our case a blue/black one (it is always black as long as there is a single value within &lt;code&gt;{n}&lt;/code&gt;). The second lower level, 1, contains two nodes and so on.&lt;/p&gt;

&lt;p&gt;In order to look up the predecessor movie for 1975 we start the binary search for the specific level where the nodes turn white by looking up the corresponding node values within the parent nodes. We find out that the level is 3. Then we traverse down along the left side and find the corresponding predecessor movie: Lolly-Maddonna XXXX (1973).&lt;/p&gt;

&lt;p&gt;What would it take to add another movie, say, &amp;ldquo;Not a Real Movie X (1975)&amp;rdquo;? That would mean we need to go through all the parent nodes and make sure they are added to the corresponding level hash functions.&lt;/p&gt;

&lt;h4 id=&#34;practical-examples:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Practical Examples&lt;/h4&gt;

&lt;p&gt;Comparing search time asymptotics for X-fast tries and search trees, we see that X-fast shines when &lt;code&gt;log[N]&lt;/code&gt; grows faster than &lt;code&gt;log[log[M]]&lt;/code&gt;, which comes down to the criteria of &lt;code&gt;N &amp;gt;&amp;gt; log[M]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s come up with some practical examples of where X-fast trie can be useful:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Imagine you are developing a flight searching website. For the given datetime of expected departure we return the list of all the upcoming flights. Flights are updated and changed frequently. Datetime minutes can make up all the possible &lt;code&gt;u&lt;/code&gt; values, and the flights to a specific direction make up &lt;code&gt;{n}&lt;/code&gt; set.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;IP/other network protocols Packet Routing: an internet router needs to redirect the IP packets to other routers with the IP closest to the requested one.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trackless bitTorrent peer-to-peer networks look up content by hash and the nodes with IDs closest to this hash (by some metric) are assigned with tracking that content. X-fast trie can get useful to lookup content in huge networks with large number of nodes (so that &lt;code&gt;n &amp;gt;&amp;gt; log[M]&lt;/code&gt;).  (Hey! By the way, see my other &lt;a href=&#34;http://www.borzov.ca/posts/kademlia/&#34;&gt;post&lt;/a&gt; on the subject!).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;data-structures-with-locality-of-reference:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;Data Structures with locality of reference&lt;/h4&gt;

&lt;p&gt;X-fast trie is an example of the data structure where locality of reference is tracked. Another one, more famous one, would of course be the search tree. Together data structures like these compose a large family of cases where some concept of locality matters. The ones used in practice tend to share many features with the x-fast trie case we considered here.&lt;/p&gt;

&lt;p&gt;It is easy to see how wide the domain of locality-sensitive applications is. It can be extended to multidimensional cases too. Say, geolocation applications would involve ability to look up objects closest to a specific point. How can one approach this? A search tree can be a good start, but for sufficiently large maps we can devise some equivalent of 2D x-fast trie or move to other fancier &lt;a href=&#34;http://en.wikipedia.org/wiki/Y-fast_trie&#34;&gt;solutions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Pretty nice, huh?&lt;/p&gt;

&lt;h4 id=&#34;see-also:58b12276bf0ae383f71846d6c29edb1d&#34;&gt;See also&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/X-fast_trie&#34;&gt;X-fast tries&lt;/a&gt; in Wikipedia&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;http://youtu.be/AjFtTQevtq0&#34;&gt;Video lecture on the subject&lt;/a&gt; from MIT OCW (the whole course is rad, totally a must watch)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>